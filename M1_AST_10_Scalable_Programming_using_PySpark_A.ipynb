{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M1_AST_10_Scalable_Programming_using_PySpark_A.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moosemaniam/IISCDeepLearning/blob/main/M1_AST_10_Scalable_Programming_using_PySpark_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGhtyFeywZ0t"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 10: PySpark Transformation on Paired RDD's and Spark Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-Sg-fnLwgwF"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yxCpv27wi2g"
      },
      "source": [
        "At the end of the experiment, you will be able to\n",
        "\n",
        "* get a better understanding of the Spark Architecture.\n",
        "\n",
        "* know the different types of transformations and action.\n",
        "\n",
        "* have a clear picture about the Narrow and Wide dependencies. \n",
        "\n",
        "* know the ways to transform the paired RDD's by applying tranformations and action on it.\n",
        "\n",
        "* have a better understanding of Data Partitioning and it's advantages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UIsPz-A6-A2"
      },
      "source": [
        "\n",
        "### Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Mf0Kd8w7BrI"
      },
      "source": [
        "#### Spark Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1s5bK-Q7BrI"
      },
      "source": [
        "* Basically Spark runs on cluster of machine.\n",
        "* It consists of master node and worker node.\n",
        "* When a spark job is initiallised master node requests the resource manage(YARN, Messos, etc.) for the worker node to implement the job.\n",
        "* Then the resource manager allots the worker nodes to the master node. Further, the master node intitlizes the Logical Plan and the data in worker nodes to process the task.\n",
        "* The Spark usually has two plans called as Logical Plan and Physical plan.\n",
        "* **Logical Plan** converts application to a dataflow of dependencies.\n",
        "\n",
        "* **Physical Plan** converts dataflow into specific tasks for execution\n",
        "of the tasks that are executed within Workers/Executors.\n",
        "\n",
        "\n",
        "  ![img](https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Workflow-of-Apache-Spark-working-with-hierarchical-index.ppm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO-1Xm5I7IPF"
      },
      "source": [
        "### RDD vs. Datasets/DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rEJDWUv7IPF"
      },
      "source": [
        "**RDD :** They are immutable distributed collection of elements.\n",
        "* Transformations, actions and partitions can be done on a RDD.\n",
        "* We can apply Lower-level abstraction.\n",
        "* We can have better control on the data when compared to DataFrames, but we need more coding.\n",
        "* We need to define the schema manually.\n",
        "* RDD is slower than Dataframes to perform simple operations like grouping the data.\n",
        "\n",
        "\n",
        "**DataFrame :** They are immutable distributed collection\n",
        "(like RDD)\n",
        "* The Data in DataFrames are organized into named columns.\n",
        "* It imposes a structure and are easier for abstraction.\n",
        "* It makes processing of large data sets easier when compared to RDD.\n",
        "* Automatically finds out the schema of the dataset.\n",
        "* Performs aggregation faster than RDDs, as it provides an easy API to perform aggregation operations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I9nckY3wmo7"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKfqepEkwSrQ"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sQsQQMKws1H"
      },
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PcfgYkSwv-F",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M1_AST_10_Scalable_Programming_using_PySpark_A\" #name of the notebook\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")  \n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa.iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cZtrZQvxLA-"
      },
      "source": [
        " **Install PySpark**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-yy_oGaxMyk"
      },
      "source": [
        "!pip -q install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6unaCynjxPYP"
      },
      "source": [
        "**Creating Spark Session**\n",
        "\n",
        "Spark session is a combined entry point of a Spark application, which came into implementation from Spark 2.0 (Instead of having various contexts, everything is encapsulated in a Spark session)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JR-BjWIxRht"
      },
      "source": [
        "# Start spark session\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf  # User Defined Functions\n",
        "from pyspark.sql.types import StringType\n",
        "from operator import add\n",
        "spark = SparkSession.builder.appName('Spark_RDD').getOrCreate()\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG1X05_bxWz-"
      },
      "source": [
        "# Accessing sparkContext from sparkSession instance.\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9XfqkyV1Rad"
      },
      "source": [
        "### Transformations:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPpwh5bX11em"
      },
      "source": [
        "**GroupByKey :** It groups all values for each and every key with in the RDD. It will create a new pair, where the original key corresponds to this collected group of values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0MHh802RnI2"
      },
      "source": [
        "1. Apply groupbykey operation to the data defined below.\n",
        "\n",
        "  data = [\n",
        "\n",
        "  ( \" USA \" , 1 ) , ( \" USA \" , 2 ) , ( \" India \" , 1 ) ,\n",
        "\n",
        "  ( \" UK \" , 1 ) , ( \" India \" , 4 ) , ( \" India \" , 9 ) ,\n",
        "       \n",
        "  ( \" USA \" , 8 ) , ( \" USA \" , 3 ) , ( \" India \" , 4 ) ,\n",
        "\n",
        "  ( \" UK \" , 6 ) , ( \" UK \" , 9 ) , ( \" UK \" , 5 )\n",
        "\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50Yxijet2-dC"
      },
      "source": [
        "# CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xidJQ6vB6V7A"
      },
      "source": [
        "**cogroup :** For each key k in self RDD or in other RDD, cogroup returns a resulting RDD that contains a tuple with the list of values for that key in self as well as other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQa5AoFrRknz"
      },
      "source": [
        "1. Group the values according to the keys for the below given data.\n",
        "  \n",
        "  data1 = [ ( 1 , \" spark \" ) , ( 2 , \" HDFS \" ) , ( 3 , \" Hive \" ) , ( 4 , \" Flink \" ) , ( 6 , \" HBase \" ) ]\n",
        "  \n",
        "  data2 = [ ( 4 , \" RealTime \" ) , ( 5 , \" Kafka \" ) , ( 6 , \" NOSQL \" ) , ( 1 , \" stream \" ) , ( 1 , \" MLlib \" ) ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rxnwK5X7nGn"
      },
      "source": [
        "# CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcs4sJF89bcK"
      },
      "source": [
        "**subtract:** The subtraction is done only to the like tuples and returns the first RDD after subtracting the values from the second RDD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzE9oTnl_zVB"
      },
      "source": [
        "1. Subtract the following data.\n",
        "\n",
        "  data1 = [ ( \" apple \" , 1 ) , ( \" banana \" , 4) , ( \" carrot \" , 5) , ( \" mango \" , 3) ]\n",
        "\n",
        "  data2 = ( [ ( \" apple \" , 1) , ( \" carrot \" , 5 ) ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFDSq5YY-lVG"
      },
      "source": [
        "# CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlrPLR9aIVjT"
      },
      "source": [
        "### JOINS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeIY-XQbHVuf"
      },
      "source": [
        "**join :** The Join is a database term. It combines the fields from two table using common values. join() operation in Spark is defined on pair-wise RDD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRX7lJMpHg-O"
      },
      "source": [
        "1. Apply join operation to the data defined below.\n",
        "\n",
        "  data = [ ( ' A ', 1 ) , ( ' b ', 2 ) , ( ' c ' , 3 ), ( ' d ' , 8 ) ] \n",
        "\n",
        "  data2 =[ ( ' A ' , 4 ) , ( ' A ' , 6 ) , ( ' b ' , 7 ) , ( ' c ' , 3 ) , ( ' c ' , 8 ) ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoMOquAUHhkO"
      },
      "source": [
        "# CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-D5SfbbHnVz"
      },
      "source": [
        "2. Apply join operation to the data given below:\n",
        "\n",
        "  data1 = [ ( \" apple \" , 1), ( \" banana \" , 4), ( \" carrot \" , 5), ( \" mango \" , 3)]\n",
        "  \n",
        "  data2 = [ ( \" apple \" , 3), ( \" banana \" , 2)]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PsiUgZTHpby"
      },
      "source": [
        "# CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO7BOlJ_oWHx"
      },
      "source": [
        "**Left Outer Join :** A left outer join returns all the values from the left table and matched values from the right table (NULL in the case of no matching join predicate)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiR7Iqlypkmf"
      },
      "source": [
        "1. Apply left outer join operation to the data defined below.\n",
        "\n",
        " data1 = [ ( \" apple \" , 1), ( \" banana \" , 4), ( \" carrot \" , 5), ( \" mango \" , 3)]\n",
        "  \n",
        "  data2 = [ ( \" apple \" , 3), ( \" banana \" , 2)]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNAEtHFhI6C4"
      },
      "source": [
        "# CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs9r6btion09"
      },
      "source": [
        "**Right Outer Join :** A right outer join returns all the values from the right table and matched values from the left table (NULL in the case of no matching join predicate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWa9denvpqAc"
      },
      "source": [
        "1. Apply right outer join operation to the data defined below.\n",
        "\n",
        "  data1 = [ ( \" apple \" , 1), ( \" banana \" , 4), ( \" carrot \" , 5), ( \" mango \" , 3)]\n",
        "  \n",
        "  data2 = [ ( \" apple \" , 3), ( \" banana \" , 2)]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Whku7e6kJX0e"
      },
      "source": [
        "# CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yqb3Dfta-dLt"
      },
      "source": [
        "### Actions on Paired RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWYIjOmB-ion"
      },
      "source": [
        "**countByKey :**  It count's the number of elements for each key."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO5P8SP1-1_p"
      },
      "source": [
        "1. For the below defined data count the keys(Using countByKey action) for each student.\n",
        "\n",
        "  data = [\n",
        "    \n",
        "  ( \" bharat \" , [ \" c \" ,  \" java \" ,  \" python \" ] ) ,\n",
        "                                      \n",
        "  ( \" chandu \" , [ \" c \" , \" java \" , \" python \" , \" scala \" ] ) ,\n",
        "                                      \n",
        "  ( \" akash \" , [ \" c \" ,  \" java \" , \" python \" , \" scala \" , \" spark \" , \" cobal \" ] ) ,\n",
        "                                      \n",
        "  ( \" bharat \" , [ \" c \" , \" java \" , \" python \" ] ) ,\n",
        "                                      \n",
        "  ( \" akash \" , [ \" c \" , \" java \" , \" python \" , \" scala \" , \" spark \" , \" cobal \" ] ) ,\n",
        "                                      \n",
        "  ( \" akash \" , [ \" c \" , \" java \" , \" python \" ) ,\n",
        "                                      \n",
        "  ( \" bharat \" , [ \" c \" , \" java \" , \" python \" ] ) ,\n",
        "                                      \n",
        "  ( \" chandu \" , [ \" c \" ,  \" java \" , \" python \" , \" scala \" ] ) ,\n",
        "                                      \n",
        "  ( \" deepak \" , [ \" c \" , \" java \" , \" python \" ] ) ,\n",
        "\n",
        "  ( \" chandu \" , [ \" c \" , \" java \" , \" python \" , \" scala \" ] ) , \n",
        "\n",
        "  ( \" deepak \" , [ \" c \" , \" java \" , \" python \" ] ) ,\n",
        "\n",
        " ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbmhuYix-3wb"
      },
      "source": [
        "# CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOCNCJiJbTpc"
      },
      "source": [
        "**collectAsMap:** It returns the RDD as a native\n",
        "Dictionary or Map object. In simple words, when a Paired RDD with (k,v) and collectAsMAp is applied on it, then a values are returned when the key is called."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbqEJuQheiGO"
      },
      "source": [
        "1. For the given below data find the values with the help key.\n",
        "\n",
        "  data = [ ( \" apple \" , 1 ) , ( \" banana \" , 4 ) , ( \" carrot \" , 5 ) , ( \" mango \" , 3 ) ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykyWa1Lmeida"
      },
      "source": [
        "# CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN1VWMevgiH4"
      },
      "source": [
        "### Narrow and Wide Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI7TQBEmQKk8"
      },
      "source": [
        "**Narrow Dependency:**-In Narrow transformation, all the elements that are required to compute the records in single partition exist in the single partition of parent RDD. A limited subset of partition is used to calculate the result. Shuffling is not done in narrow transformation.\n",
        "\n",
        "Example:- map, filter, flatmap, mappartition, sample, union, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eku2VnfK9KW3"
      },
      "source": [
        "\n",
        "\n",
        "* In Narrow transformation, all the elements that are required to compute the records in single partition should be in the single partition of parent RDD.\n",
        "* A limited subset of partition is used to calculate the result.\n",
        "* Each output partition depends on exactly one or a few input partitions in RDD.\n",
        "* Each input partition is used by exactly one\n",
        "output partition.\n",
        "* So output and input partitions can be on same Worker.\n",
        "* It is also called as full dependency.\n",
        "* Narrow transformations are the result of map(), filter().\n",
        "\n",
        "\n",
        "![img](https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/narrow_transformation.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zti-0hZqXLFb"
      },
      "source": [
        "**Wide Dependency:-** In wide transformation, all the elements that are required to compute the records in the single partition may exist in many partitions of parent RDD. In simple terms we can say that shuffling is done in wide transformations.\n",
        "\n",
        "Example:- Intersection, distinct, reducebykey, groupbykey, join, cartesian,coalese, repartition, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5lFyIg5-RoD"
      },
      "source": [
        "\n",
        "\n",
        "* In wide transformation, all the elements that are required to compute the records in the single partition may be in many partitions of parent RDD.\n",
        "* Each input partition can be used by one or more output partitions RDD.\n",
        "* So, wide dependency forces a shuffle across Workers.\n",
        "* This is also called as Shuffle or partial dependency.\n",
        "* Wide Dependency are the result of groupbyKey and reducebyKey.\n",
        "\n",
        "\n",
        "\n",
        "  ![img](https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/wide_trans.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10jrSv6sgkIq"
      },
      "source": [
        "### Data Partitioning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdBeyVjYgp9S"
      },
      "source": [
        "**Data Partitioning :** \n",
        "\n",
        "*    RDD's are spread on partitions across nodes of a\n",
        "cluster.\n",
        "*    Controlling the number of partitions, helps balance compute load, and helps to reduce shuffle communication across nodes.\n",
        "* Can also be used for specific algorithms to operate on partition at a time.\n",
        "* Spark tracks partitioner used to generate an RDD to optimize operations.\n",
        "* Many transformations can set or unset the number of partitiones like Map following a Join can unset Hash partitioner and Filter, mapValue & flatMapValues retain partitioner.\n",
        "\n",
        "The Default Partitioners in Pyspark are:-\n",
        "* HashPartitioner: It is uses keys or values to partitions by using an aggregation function on it. This is mainly used in groupby, reduceby etc.\n",
        "* RangePartitioner: It creates roughly equal ranges,\n",
        "determined by sampling the RDD contents. It is mainly used in orderby."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8t1IW7fPdXr"
      },
      "source": [
        "**mapPartitions:** It is similar to map() operation where the output of mapPartitions() returns the same number of rows as in input RDD.\n",
        "It is used to improve the performance of the map() when there is a need to do heavy initializations like Database connection.\n",
        "mapPartitions() applies a heavy initialization to each partition of RDD instead of each element of RDD.\n",
        "It is a Narrow transformation operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n9I03-BXWwa"
      },
      "source": [
        "1. Find the sum of each partition in the below defined RDD.\n",
        "\n",
        "  data = [74, 45, 87, 95] with 2 partitions in it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byiQ8-lVkmxp"
      },
      "source": [
        "# CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAFq_TVVSYPH"
      },
      "source": [
        "Here we can see that the sum operation has been applied to the partitions itself rather than the entire RDD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wxhxrCVX22f"
      },
      "source": [
        "**mapPartitionsWithIndex() :** It iterates all over the \n",
        "elements in a partition and also gives access to\n",
        "partition number and partition number would also be taken as the input (as a Broadcast variable)and the function is applied on it.\n",
        "MapPartitionsWithIndex is similar to mapPartitions, except that it takes one more argument as input, which is the index of the partition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTZAd3wLdgz4"
      },
      "source": [
        "1. Find the sum of the indexes of the partitions in the below defined data.\n",
        "\n",
        "  data = [74, 45, 87, 95] with 4 partitions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHsOvibZd2Fw"
      },
      "source": [
        "# CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bfl3ZNM318OQ"
      },
      "source": [
        "num partition and get partitioner\n",
        "for each partition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57nKm2Dp-mpE"
      },
      "source": [
        "**foreachPartition :** It works in the same way as the foreach. Only difference is that it applies a function to each partition of the RDD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVLk2dHW-7lH"
      },
      "source": [
        "1. Apply foreachPartition on the below defined data\n",
        "  \n",
        "  data = [ 19 , 42 , 73 , 24 , 54 ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYcFOscA-M8O"
      },
      "source": [
        "# CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re3fV6cWeguq"
      },
      "source": [
        "### Numeric RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFxfN3ZKei9W"
      },
      "source": [
        "* It is commonly used in statistics for RDDs having numeric type.\n",
        "*  stats() action to populate all stats of a RDD.\n",
        "* Individual functions (actions) also available such as count, mean,  mode, variance, standard deviation, min , max, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CwFueTUgTNJ"
      },
      "source": [
        "1. Find all the stats, count, mean , mode, variance, standard deviation, min , max for the below defined RDD.\n",
        "\n",
        "  data = [ 2, 3, 7, 1, 9, 17, 28, 37, 98, 72]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sIJ8uqkgqbh"
      },
      "source": [
        "# CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0fE8R6Ud-s8"
      },
      "source": [
        "#@title Q.1.Which of the following is a transformation?  { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer1 = \"\" #@param [\"\",\"take(n)\", \"top()\", \"countByValue()\",\"mapPartitionWithIndex()\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlSp1X_GeF41"
      },
      "source": [
        "#@title Q.2.  What is a transformation in Spark RDD?{ run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer2 = \"\" #@param [\"\",\"Takes RDD as input and produces one or more RDD as output.\", \"Returns final result of RDD computations.\",\"The ways to send result from executors to the driver\",\"None of the above\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}